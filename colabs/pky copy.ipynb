{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih5NlvdDEOI1",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhfMmVdHoZM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Preprocessing functions from data_util.py in SimCLR repository (hidden).\n",
        "\n",
        "FLAGS_color_jitter_strength = 0.3\n",
        "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
        "\n",
        "\n",
        "def random_apply(func, p, x):\n",
        "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
        "  return tf.cond(\n",
        "      tf.less(tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "              tf.cast(p, tf.float32)),\n",
        "      lambda: func(x),\n",
        "      lambda: x)\n",
        "\n",
        "\n",
        "def random_brightness(image, max_delta, impl='simclrv2'):\n",
        "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
        "  if impl == 'simclrv2':\n",
        "    factor = tf.random_uniform(\n",
        "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
        "    image = image * factor\n",
        "  elif impl == 'simclrv1':\n",
        "    image = random_brightness(image, max_delta=max_delta)\n",
        "  else:\n",
        "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
        "  return image\n",
        "\n",
        "\n",
        "def to_grayscale(image, keep_channels=True):\n",
        "  image = tf.image.rgb_to_grayscale(image)\n",
        "  if keep_channels:\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "def color_jitter(image,\n",
        "                 strength,\n",
        "                 random_order=True):\n",
        "  \"\"\"Distorts the color of the image.\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    strength: the floating number for the strength of the color augmentation.\n",
        "    random_order: A bool, specifying whether to randomize the jittering order.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  brightness = 0.8 * strength\n",
        "  contrast = 0.8 * strength\n",
        "  saturation = 0.8 * strength\n",
        "  hue = 0.2 * strength\n",
        "  if random_order:\n",
        "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
        "  else:\n",
        "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
        "\n",
        "\n",
        "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      if brightness != 0 and i == 0:\n",
        "        x = random_brightness(x, max_delta=brightness)\n",
        "      elif contrast != 0 and i == 1:\n",
        "        x = tf.image.random_contrast(\n",
        "            x, lower=1-contrast, upper=1+contrast)\n",
        "      elif saturation != 0 and i == 2:\n",
        "        x = tf.image.random_saturation(\n",
        "            x, lower=1-saturation, upper=1+saturation)\n",
        "      elif hue != 0:\n",
        "        x = tf.image.random_hue(x, max_delta=hue)\n",
        "      return x\n",
        "\n",
        "    for i in range(4):\n",
        "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is random).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      def brightness_foo():\n",
        "        if brightness == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return random_brightness(x, max_delta=brightness)\n",
        "      def contrast_foo():\n",
        "        if contrast == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
        "      def saturation_foo():\n",
        "        if saturation == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_saturation(\n",
        "              x, lower=1-saturation, upper=1+saturation)\n",
        "      def hue_foo():\n",
        "        if hue == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_hue(x, max_delta=hue)\n",
        "      x = tf.cond(tf.less(i, 2),\n",
        "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
        "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
        "      return x\n",
        "\n",
        "    perm = tf.random_shuffle(tf.range(4))\n",
        "    for i in range(4):\n",
        "      image = apply_transform(perm[i], image)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _compute_crop_shape(\n",
        "    image_height, image_width, aspect_ratio, crop_proportion):\n",
        "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
        "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
        "  less than or equal to `crop_proportion` along the other side.\n",
        "  Args:\n",
        "    image_height: Height of image to be cropped.\n",
        "    image_width: Width of image to be cropped.\n",
        "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    crop_height: Height of image after cropping.\n",
        "    crop_width: Width of image after cropping.\n",
        "  \"\"\"\n",
        "  image_width_float = tf.cast(image_width, tf.float32)\n",
        "  image_height_float = tf.cast(image_height, tf.float32)\n",
        "\n",
        "  def _requested_aspect_ratio_wider_than_image():\n",
        "    crop_height = tf.cast(tf.rint(\n",
        "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * image_width_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  def _image_wider_than_requested_aspect_ratio():\n",
        "    crop_height = tf.cast(\n",
        "        tf.rint(crop_proportion * image_height_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.rint(\n",
        "        crop_proportion * aspect_ratio *\n",
        "        image_height_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  return tf.cond(\n",
        "      aspect_ratio > image_width_float / image_height_float,\n",
        "      _requested_aspect_ratio_wider_than_image,\n",
        "      _image_wider_than_requested_aspect_ratio)\n",
        "\n",
        "\n",
        "def center_crop(image, height, width, crop_proportion):\n",
        "  \"\"\"Crops to center of image and rescales to desired size.\n",
        "  Args:\n",
        "    image: Image Tensor to crop.\n",
        "    height: Height of image to be cropped.\n",
        "    width: Width of image to be cropped.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "  \"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  offset_height = ((image_height - crop_height) + 1) // 2\n",
        "  offset_width = ((image_width - crop_width) + 1) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image: `Tensor` of image data.\n",
        "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
        "        where each coordinate is [0, 1) and the coordinates are arranged\n",
        "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
        "        image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "        area of the image must contain at least this fraction of any bounding\n",
        "        box supplied.\n",
        "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
        "        image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `float`s. The cropped area of the image\n",
        "        must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "        region of the image of the specified constraints. After `max_attempts`\n",
        "        failures, return the entire image.\n",
        "    scope: Optional `str` for name scope.\n",
        "  Returns:\n",
        "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    shape = tf.shape(image)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image, offset_y, offset_x, target_height, target_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def crop_and_resize(image, height, width):\n",
        "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
        "  Args:\n",
        "    image: Tensor representing the image.\n",
        "    height: Desired image height.\n",
        "    width: Desired image width.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
        "  \"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  aspect_ratio = width / height\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=100,\n",
        "      scope=None)\n",
        "  return tf.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  \"\"\"Blurs the given image with separable convolution.\n",
        "  Args:\n",
        "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
        "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
        "      be an odd number. If it is an even number, the actual kernel size will be\n",
        "      size + 1.\n",
        "    sigma: Sigma value for gaussian operator.\n",
        "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
        "  Returns:\n",
        "    A Tensor representing the blurred image.\n",
        "  \"\"\"\n",
        "  radius = tf.to_int32(kernel_size / 2)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
        "    # an extra dimension.\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "def random_crop_with_resize(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly crop and resize an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: Probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  def _transform(image):  # pylint: disable=missing-docstring\n",
        "    image = crop_and_resize(image, height, width)\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_color_jitter(image, p=1.0):\n",
        "  def _transform(image):\n",
        "    color_jitter_t = functools.partial(\n",
        "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
        "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
        "    return random_apply(to_grayscale, p=0.2, x=image)\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_blur(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly blur an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  del width\n",
        "  def _transform(image):\n",
        "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
        "    return gaussian_blur(\n",
        "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
        "  \"\"\"Apply efficient batch data transformations.\n",
        "  Args:\n",
        "    images_list: a list of image tensors.\n",
        "    height: the height of image.\n",
        "    width: the width of image.\n",
        "    blur_probability: the probaility to apply the blur operator.\n",
        "  Returns:\n",
        "    Preprocessed feature list.\n",
        "  \"\"\"\n",
        "  def generate_selector(p, bsz):\n",
        "    shape = [bsz, 1, 1, 1]\n",
        "    selector = tf.cast(\n",
        "        tf.less(tf.random_uniform(shape, 0, 1, dtype=tf.float32), p),\n",
        "        tf.float32)\n",
        "    return selector\n",
        "\n",
        "  new_images_list = []\n",
        "  for images in images_list:\n",
        "    images_new = random_blur(images, height, width, p=1.)\n",
        "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
        "    images = images_new * selector + images * (1 - selector)\n",
        "    images = tf.clip_by_value(images, 0., 1.)\n",
        "    new_images_list.append(images)\n",
        "\n",
        "  return new_images_list\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width,\n",
        "                         color_distort=True, crop=True, flip=True):\n",
        "  \"\"\"Preprocesses the given image for training.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    color_distort: Whether to apply the color distortion.\n",
        "    crop: Whether to crop the image.\n",
        "    flip: Whether or not to flip left and right of an image.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = random_crop_with_resize(image, height, width)\n",
        "  if flip:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  if color_distort:\n",
        "    image = random_color_jitter(image)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width, crop=True):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    crop: Whether or not to (center) crop the test images.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width, is_training=False,\n",
        "                     color_distort=True, test_crop=True):\n",
        "  \"\"\"Preprocesses the given image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    is_training: `bool` for whether the preprocessing is for training.\n",
        "    color_distort: whether to apply the color distortion.\n",
        "    test_crop: whether or not to extract a central crop of the images\n",
        "        (as for standard ImageNet evaluation) during the evaluation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor` of range [0, 1].\n",
        "  \"\"\"\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, color_distort)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width, test_crop)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDCY4h7bHxj8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202,
          "referenced_widgets": [
            "ed71033be23e46df83647e682528d7bc",
            "d6b19f0a1f824b229fe8fedf02f232ed",
            "843fc6861cc049198d73152839c75d09",
            "0e90d689153149d9a11d6da6400bd0ea",
            "fe61a9d20be54db3841f94bb1ae67dd1",
            "798252693b3c4afeb39103ce9768be88",
            "39f878b38c684abd816ac500f940d142",
            "7afc77b8c06a4c2cafa1495daf4b772b"
          ]
        },
        "outputId": "dd6d1b2b-4ed8-445f-8d40-eb2b6ee1c3b4",
        "tags": []
      },
      "source": [
        "#@title Load tensorflow datasets: we use tensorflow flower dataset as an example\n",
        "\n",
        "batch_size = 5\n",
        "dataset_name = 'tf_flowers'\n",
        "\n",
        "tfds_dataset, tfds_info = tfds.load(\n",
        "    dataset_name, split='train', with_info=True)\n",
        "num_images = tfds_info.splits['train'].num_examples\n",
        "num_classes = tfds_info.features['label'].num_classes\n",
        "\n",
        "def _preprocess(x):\n",
        "  x['image'] = preprocess_image(\n",
        "      x['image'], 224, 224, is_training=False, color_distort=False)\n",
        "  return x\n",
        "x = tfds_dataset.map(_preprocess).batch(batch_size)\n",
        "x = tf.data.make_one_shot_iterator(x).get_next()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WXspghpERRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "61ef8760-69dc-4393-a4c7-ed048f9c11e5",
        "tags": []
      },
      "source": [
        "#@title Load module and get the computation graph\n",
        "\n",
        "hub_path = 'gs://simclr-checkpoints/simclrv2/finetuned_100pct/r50_1x_sk0/hub/'\n",
        "module = hub.Module(hub_path, trainable=False)\n",
        "key = module(inputs=x['image'], signature=\"default\", as_dict=True)\n",
        "logits_t = key['logits_sup'][:, :]\n",
        "key # The accessible tensor in the return dictionary"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\nINFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'block_group2': <tf.Tensor 'module_apply_default/base_model/block_group2:0' shape=(?, 28, 28, 512) dtype=float32>,\n 'default': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(?, 2048) dtype=float32>,\n 'block_group4': <tf.Tensor 'module_apply_default/base_model/block_group4:0' shape=(?, 7, 7, 2048) dtype=float32>,\n 'block_group1': <tf.Tensor 'module_apply_default/base_model/block_group1:0' shape=(?, 56, 56, 256) dtype=float32>,\n 'final_avg_pool': <tf.Tensor 'module_apply_default/base_model/final_avg_pool:0' shape=(?, 2048) dtype=float32>,\n 'logits_sup': <tf.Tensor 'module_apply_default/head_supervised/linear_layer/linear_layer_out:0' shape=(?, 1000) dtype=float32>,\n 'initial_max_pool': <tf.Tensor 'module_apply_default/base_model/initial_max_pool:0' shape=(?, 56, 56, 64) dtype=float32>,\n 'initial_conv': <tf.Tensor 'module_apply_default/base_model/initial_conv:0' shape=(?, 112, 112, 64) dtype=float32>,\n 'block_group3': <tf.Tensor 'module_apply_default/base_model/block_group3:0' shape=(?, 14, 14, 1024) dtype=float32>}"
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHj1FZ2dEXIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2IRXn12EZFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image, labels, logits = sess.run((x['image'], x['label'], logits_t))\n",
        "pred = logits.argmax(-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Populating the interactive namespace from numpy and matplotlib\n"
        }
      ],
      "source": [
        "# https://e3oroush.github.io/tsne-visualization/\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.manifold import TSNE\n",
        "import os, re, glob2, pickle\n",
        "\n",
        "from keras.engine import  Model\n",
        "from keras.layers import Input\n",
        "# from keras_vggface.vggface import VGGFace\n",
        "from keras.preprocessing import image\n",
        "# from keras_vggface import utils\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# custom paramers: change these parameters to properly run on your machine\n",
        "image_path = '/home/esoroush/Datasets/MSRC/MSRC/' # addres of images\n",
        "no_of_images = 1600 # number of images. It is recommended to use a square of 2 number\n",
        "ellipside =False # elipsoid or rectangular visualization\n",
        "image_width = 64 # width and height of each visualized images\n",
        "# choices are: inception, raw and vggfaces\n",
        "feature_extraction = 'inception' # feature extraction method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find all images\n",
        "image_names  = glob2.glob(image_path + \"**/*.png\") \n",
        "image_names +=glob2.glob(image_path + \"**/*.jpg\")\n",
        "image_names +=glob2.glob(image_path + \"**/*.gif\")\n",
        "# suffle images\n",
        "np.random.seed(3)\n",
        "np.random.shuffle(image_names)\n",
        "if no_of_images > len(image_names):\n",
        "    no_of_images = len(image_names)\n",
        "image_names = image_names[:no_of_images]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google inception pre-trained network\n",
        "if feature_extraction == 'inception':\n",
        "    print('using %s network/method for feature extraction'%feature_extraction)\n",
        "    import sys, tarfile\n",
        "    from six.moves import urllib\n",
        "    model_dir = os.path.join(os.environ['HOME'], '.tensorflow/models')\n",
        "    DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
        "    def create_graph():\n",
        "\n",
        "        \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
        "        # Creates graph from saved graph_def.pb.\n",
        "        with tf.gfile.FastGFile(os.path.join(\n",
        "          model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
        "            graph_def = tf.GraphDef()\n",
        "            graph_def.ParseFromString(f.read())\n",
        "            _ = tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "    def run_inference_on_image(image):\n",
        "\n",
        "        \"\"\"Runs forward path on an image.\n",
        "        Args:\n",
        "        image: Image file name.\n",
        "\n",
        "        Returns:\n",
        "        off the shelf 2048 feature vector\n",
        "        \"\"\"\n",
        "        if not tf.gfile.Exists(image):\n",
        "            tf.logging.fatal('File does not exist %s', image)\n",
        "        image_data = tf.gfile.FastGFile(image, 'rb').read()\n",
        "\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "        # Some useful tensors:\n",
        "        # 'softmax:0': A tensor containing the normalized prediction across\n",
        "        #   1000 labels.\n",
        "        # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n",
        "        #   float description of the image.\n",
        "        # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n",
        "        #   encoding of the image.\n",
        "        # Runs the softmax tensor by feeding the image_data as input to the graph.\n",
        "            pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
        "            features = sess.run(pool3,\n",
        "                                   {'DecodeJpeg/contents:0': image_data})\n",
        "            return features\n",
        "    \n",
        "    def maybe_download_and_extract():\n",
        "        \"\"\"Download and extract model tar file.\"\"\"\n",
        "        dest_directory = model_dir\n",
        "        if not os.path.exists(dest_directory):\n",
        "            os.makedirs(dest_directory)\n",
        "        filename = DATA_URL.split('/')[-1]\n",
        "        filepath = os.path.join(dest_directory, filename)\n",
        "        if not os.path.exists(filepath):\n",
        "            def _progress(count, block_size, total_size):\n",
        "                sys.stdout.write('\\r>> Downloading %s %.1f%%' % ( \n",
        "                      filename, float(count * block_size) / float(total_size) * 100.0))\n",
        "                sys.stdout.flush()\n",
        "            filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
        "            print()\n",
        "            statinfo = os.stat(filepath)\n",
        "            print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
        "        tarfile.open(filepath, 'r:gz').extractall(dest_directory)\n",
        "\n",
        "\n",
        "    maybe_download_and_extract()\n",
        "    # Creates graph from saved GraphDef.\n",
        "    create_graph()\n",
        "    feature_filename = '%s-feature-inception-%d.p'%(image_path.split('/')[-2], no_of_images)\n",
        "    if os.path.exists(feature_filename):\n",
        "        with open(feature_filename, 'rb') as f:\n",
        "            features, image_names = pickle.load(f)\n",
        "    else:\n",
        "        features = np.zeros([no_of_images, 2048])\n",
        "        for i in xrange(no_of_images):\n",
        "            print('image name: %s index: %d/%d' %(image_names[i], i, no_of_images))\n",
        "            features[i, :] = run_inference_on_image(image=image_names[i]).squeeze()\n",
        "        with open(feature_filename, 'wb') as f:\n",
        "            pickle.dump((features, image_names), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raw image pixels resized to 100x100\n",
        "if feature_extraction == 'raw':\n",
        "    print('using %s network/method for feature extraction'%feature_extraction)\n",
        "    features = np.zeros([no_of_images, 100*100])\n",
        "    for i, name in enumerate(image_names):\n",
        "        features[i, :] = np.asarray(Image.open(name).resize((100, 100)).convert('L')).reshape(-1,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vgg face pretrained network\n",
        "if feature_extraction == 'vggfaces':\n",
        "    print('using %s network/method for feature extraction'%feature_extraction)\n",
        "    # Convolution Features\n",
        "    features = np.zeros([no_of_images, 2048])\n",
        "    vgg_model_conv = VGGFace(include_top=False, input_shape=(224, 224, 3), pooling='avg') # pooling: None, avg or max\n",
        "    # FC7 Features\n",
        "    vgg_model = VGGFace() # pooling: None, avg or max\n",
        "    out = vgg_model.get_layer('fc7').output\n",
        "    vgg_model_fc7 = Model(vgg_model.input, out)\n",
        "\n",
        "    feature_filename = '%s-feature-vggfaces-%d.p'%(image_path.split('/')[-2], no_of_images)\n",
        "    if os.path.exists(feature_filename):\n",
        "        with open(feature_filename, 'rb') as f:\n",
        "            features, image_names = pickle.load(f)\n",
        "    else:\n",
        "        features = np.zeros([no_of_images, 4096])\n",
        "        for i, name in enumerate(image_names):\n",
        "            img = image.load_img(name, target_size=(224, 224))\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "            x = utils.preprocess_input(x)\n",
        "            print('image name: %s progress: %d/%d'%(name, i, no_of_images))\n",
        "            features[i, :] = vgg_model_fc7.predict(x)\n",
        "        with open(feature_filename, 'wb') as f:\n",
        "            pickle.dump((features, image_names), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use tsne to cluster images in 2 dimensions\n",
        "tsne = TSNE()\n",
        "reduced = tsne.fit_transform(features)\n",
        "reduced_transformed = reduced - np.min(reduced, axis=0)\n",
        "reduced_transformed /= np.max(reduced_transformed, axis=0)\n",
        "image_xindex_sorted = np.argsort(np.sum(reduced_transformed, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# draw all images in a merged image\n",
        "merged_width = int(np.ceil(np.sqrt(no_of_images))*image_width)\n",
        "merged_image = np.zeros((merged_width, merged_width, 3), dtype='uint8')\n",
        "\n",
        "for counter, index in enumerate(image_xindex_sorted):\n",
        "    # set location\n",
        "    if ellipside:\n",
        "        a = np.ceil(reduced_transformed[counter, 0] * (merged_width-image_width-1)+1)\n",
        "        b = np.ceil(reduced_transformed[counter, 1] * (merged_width-image_width-1)+1)\n",
        "        a = int(a - np.mod(a-1,image_width) + 1)\n",
        "        b = int(b - np.mod(b-1,image_width) + 1)\n",
        "        if merged_image[a,b,0] != 0:\n",
        "            continue\n",
        "        image_address = image_names[counter]\n",
        "        img = np.asarray(Image.open(image_address).resize((image_width, image_width)))\n",
        "        merged_image[a:a+image_width, b:b+image_width,:] = img[:,:,:3]\n",
        "    else:\n",
        "        b = int(np.mod(counter, np.sqrt(no_of_images)))\n",
        "        a = int(np.mod(counter//np.sqrt(no_of_images), np.sqrt(no_of_images)))\n",
        "        image_address = image_names[index]\n",
        "        img = np.asarray(Image.open(image_address).resize((image_width, image_width)))\n",
        "        merged_image[a*image_width:(a+1)*image_width, b*image_width:(b+1)*image_width,:] = img[:,:,:3]\n",
        "\n",
        "plt.imshow(merged_image)\n",
        "plt.show()\n",
        "merged_image = Image.fromarray(merged_image)\n",
        "if ellipside:\n",
        "    merged_image.save('merged-%s-ellipsoide-inception.png'%image_path.split('/')[-2])\n",
        "else:\n",
        "    merged_image.save('merged-%s.png'%image_path.split('/')[-2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[1. 2.]\n [2. 6.]\n [3. 9.]\n [9. 3.]\n [1. 5.]]\n[[0.         0.        ]\n [0.125      0.57142857]\n [0.25       1.        ]\n [1.         0.14285714]\n [0.         0.42857143]]\n"
        }
      ],
      "source": [
        "a = np.array([[1,2], [2,6], [3,9], [9,3], [1,5]], dtype=float)\n",
        "print(a)\n",
        "\n",
        "normalized_a = a - np.min(a, axis=0)\n",
        "normalized_a /= np.max(normalized_a, axis=0)\n",
        "print(normalized_a)\n",
        "# reduced_transformed = reduced - np.min(reduced, axis=0)\n",
        "# reduced_transformed /= np.max(reduced_transformed, axis=0)\n",
        "# image_xindex_sorted = np.argsort(np.sum(reduced_transformed, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--2020-07-08 14:08:17--  http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz\nResolving ai.stanford.edu (ai.stanford.edu)...171.64.68.10\nConnecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80...connected.\nHTTP request sent, awaiting response...200 OK\nLength: 2640397119 (2.5G) [application/x-gzip]\nSaving to: ‘stl10_binary.tar.gz’\n\nstl10_binary.tar.gz 100%[===================>]   2.46G   622KB/s    in 30m 36s \n\n2020-07-08 14:38:54 (1.37 MB/s) - ‘stl10_binary.tar.gz’ saved [2640397119/2640397119]\n\nstl10_binary/\nstl10_binary/test_X.bin\nstl10_binary/test_y.bin\nstl10_binary/train_X.bin\nstl10_binary/train_y.bin\nstl10_binary/unlabeled_X.bin\nstl10_binary/class_names.txt\nstl10_binary/fold_indices.txt\nextracted\nfinetuning.ipynb\nilsvrc2012_wordnet_lemmas.txt\nilsvrc2012_wordnet_lemmas.txt.1\nilsvrc2012_wordnet_lemmas.txt.2\nilsvrc2012_wordnet_lemmas.txt.3\nilsvrc2012_wordnet_lemmas.txt.4\nimag-net.org_chal_LSVR_2012_nnou_ILSV_img_25sueluaBHPwWruxs2spkUscZSfqDtuIKXvsaVMgUr8.tar.tmp.088c6e19f6c94d0bbb4bcbf62b25d5c7\nimag-net.org_chal_LSVR_2012_nnou_ILSV_img_2cT1JH5yNiN96TKjDn5AFEtHe9APHM4l85B76lv0igc.tar.tmp.4afbdc6ccbe949879d01f30c69c9162d\nload_and_inference.ipynb\npky copy.ipynb\npky.ipynb\nstl10_binary\nstl10_binary.tar.gz\n"
        }
      ],
      "source": [
        "# download and extract stl10\n",
        "!wget http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz\n",
        "!tar -zxvf stl10_binary.tar.gz\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import yaml\n",
        "# import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "import importlib.util\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "load_and_inference.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python37764bittf1conda98438329aa7343f286aada627438d4ea",
      "display_name": "Python 3.7.7 64-bit ('tf1': conda)"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed71033be23e46df83647e682528d7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6b19f0a1f824b229fe8fedf02f232ed",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_843fc6861cc049198d73152839c75d09",
              "IPY_MODEL_0e90d689153149d9a11d6da6400bd0ea"
            ]
          }
        },
        "d6b19f0a1f824b229fe8fedf02f232ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "843fc6861cc049198d73152839c75d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fe61a9d20be54db3841f94bb1ae67dd1",
            "_dom_classes": [],
            "description": "Dl Completed...: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_798252693b3c4afeb39103ce9768be88"
          }
        },
        "0e90d689153149d9a11d6da6400bd0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39f878b38c684abd816ac500f940d142",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5/5 [00:04&lt;00:00,  1.19 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7afc77b8c06a4c2cafa1495daf4b772b"
          }
        },
        "fe61a9d20be54db3841f94bb1ae67dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "798252693b3c4afeb39103ce9768be88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39f878b38c684abd816ac500f940d142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7afc77b8c06a4c2cafa1495daf4b772b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}